{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deribit market agent notebook v0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, time, datetime, os, copy\n",
    "import datetime as dt\n",
    "from api_client import Client\n",
    "from historical_data import get_historical_data\n",
    "from metric_logger import MetricLogger\n",
    "import nest_asyncio\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise trading client\n",
    "client = Client()\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エージェント neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SelectItem(nn.Module):\n",
    "#     \"\"\"\n",
    "#     for picking out an element from a tuple/list\n",
    "#     at index item_index, for any layer which outputs such data\n",
    "#     \"\"\"\n",
    "#     def __init__(self, item_index):\n",
    "#         super().__init__()\n",
    "#         self._name = 'selectitem'\n",
    "#         self.item_index = torch.as_tensor(item_index)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         x, _ = inputs\n",
    "#         return x\n",
    "\n",
    "\n",
    "class Agent47Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Training model\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_dim, out_channels=100, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=100, out_channels=50, kernel_size=2, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(0,1)\n",
    "        )\n",
    "        self.online_rnn = nn.LSTM(input_size=32, num_layers=3, hidden_size=self.batch_size),\n",
    "        self.online_out = nn.Sequential(\n",
    "            nn.Flatten(0,2),\n",
    "            nn.Linear(38400, output_dim)\n",
    "        )\n",
    "        \n",
    "        #frozen target Q\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "        self.target_rnn = copy.deepcopy(self.online_rnn)\n",
    "        self.target_out = copy.deepcopy(self.online_out)\n",
    "        \n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False   \n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, model):\n",
    "        inputs = inputs.float()\n",
    "        x = inputs\n",
    "        if model == 'online':\n",
    "            x = self.online(x)\n",
    "            x, _ = self.online_rnn(x)\n",
    "            x = self.online_out(x)\n",
    "            return x\n",
    "        elif model == 'target':\n",
    "            x = self.target(x)\n",
    "            x, _ = self.target_rnn(x)\n",
    "            x = self.target_out(x)\n",
    "            return x\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エージェント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent47:\n",
    "    def __init__(self, state_dim, action_dim, save_dir, load=False, load_file=''):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        self.load = load\n",
    "        \n",
    "        self.exploration_rate = 1\n",
    "        self.decay_rate = 0.9995\n",
    "        self.min_rate = 0.15\n",
    "        self.curr_step = 0\n",
    "        \n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 16\n",
    "        \n",
    "        self.learn_every = 1  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 50  # no. of experiences between Q_target & Q_online sync\n",
    "        self.save_every = 250 #no. of experiences between saving the model\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = Agent47Net(self.state_dim, self.action_dim, self.batch_size)\n",
    "        self.net = self.net.to(device)\n",
    "                \n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        \n",
    "        if self.load:\n",
    "            checkpoint = torch.load(load_file)\n",
    "            self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.exploration_rate = checkpoint['exploration_rate']\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Return the action to do at given state\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_id = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = state.__array__() \n",
    "            state = torch.tensor(state).cuda()\n",
    "            state = state.unsqueeze(0)\n",
    "            state = torch.cat(self.batch_size * [state])\n",
    "            print(\"act shape\", state.shape)\n",
    "            action_vals = self.net(state, model=\"online\") #action scores from NN\n",
    "            action_id = torch.argmax(action_vals, axis=0).item() #value of action with highest score\n",
    "        \n",
    "        if self.exploration_rate >= self.min_rate:\n",
    "            self.exploration_rate -= 1-self.decay_rate #linear decay\n",
    "        else:\n",
    "            self.exploration_rate = self.min_rate\n",
    "        \n",
    "        self.curr_step += 1\n",
    "        return action_id #action to take at this step\n",
    "    \n",
    "    def cache(self, state, next_state, action, reward):\n",
    "        \"\"\"\n",
    "        Store experience to memory\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "        \n",
    "        state = torch.tensor(state).cuda()\n",
    "        next_state = torch.tensor(next_state).cuda()\n",
    "        action = torch.tensor([action]).cuda()\n",
    "        reward = torch.tensor([reward]).cuda()\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward))\n",
    "    \n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve/\"remember\" experiences\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze()       \n",
    "    \n",
    "    def td_estimate(self, state, action):\n",
    "        \"\"\"\n",
    "        temporal difference Q estimate at current state\n",
    "        \"\"\"\n",
    "        current_Q = self.net(state, model=\"online\")[action]\n",
    "        \n",
    "        return current_Q\n",
    "\n",
    "    def td_target(self, reward, next_state):\n",
    "        \"\"\"\n",
    "        get td target\n",
    "        \"\"\"\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=0)\n",
    "        next_Q = self.net(next_state, model=\"target\")[best_action]\n",
    "        return (reward + (self.gamma * next_Q).float())     \n",
    "    \n",
    "    def update_Q(self, td_estimate, td_target):\n",
    "        \"\"\"\n",
    "        update parameters\n",
    "        \"\"\"\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        #self.net.train()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        #self.net.eval()\n",
    "        return loss.item()\n",
    "    \n",
    "    def sync_Q_target(self):\n",
    "        \"\"\"\n",
    "        sync Q target with online periodically instead of backpropagation\n",
    "        \"\"\"\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        save model in directory\n",
    "        \"\"\"\n",
    "        now = dt.datetime.now().strftime('%B-%d-%Y')\n",
    "        save_path = (\n",
    "            self.save_dir / f\"saved_agents/with_position/{now}/agent47_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state_dict': self.net.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'exploration_rate': self.exploration_rate\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"Agent47 saved to {save_path} at step {self.curr_step}\")\n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Learning steps\n",
    "        \"\"\"\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "            \n",
    "        if self.curr_step < 4 * self.batch_size:\n",
    "            return None, None\n",
    "        \n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state)\n",
    "        \n",
    "        # Backpropagate loss through Q_online \n",
    "        loss = self.update_Q(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss) #(estimate of Q, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "NVIDIA GeForce RTX 3080\n",
      "Device total memory: 9.75 GB\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled=False\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"Device total memory: {} GB\".format(round(t/1024**3,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(equity1):\n",
    "    \"\"\"\n",
    "    return reward value\n",
    "    \"\"\"\n",
    "    summary = client.get_account_summary()\n",
    "    equity2 = float(summary[\"equity\"])\n",
    "    return (equity2-equity1)\n",
    "    \n",
    "def get_state():\n",
    "    \"\"\"\n",
    "    get state at time of function call\n",
    "    currently only order book\n",
    "    \"\"\"\n",
    "    curr_time = round(time.time())\n",
    "    now = dt.datetime.now().strftime('%B %d, %Y %H:%M:%S')\n",
    "    tick_data = get_historical_data(t1=curr_time-120, t2=curr_time)\n",
    "    order_book = client.get_order_book(instrument='BTC-PERPETUAL', depth=60)\n",
    "    position_size = client.get_position()[\"size\"]\n",
    "    open_orders = client.get_open_orders()\n",
    "    bids = order_book[\"bids\"]\n",
    "    asks = order_book[\"asks\"]\n",
    "    position_and_orders = [[position_size, 0]] * 60\n",
    "    for i in range(len(open_orders)):\n",
    "        position_and_orders[i][1] = open_orders[\"profit_loss\"]\n",
    "    \n",
    "    return [bids, asks, position_and_orders]\n",
    "\n",
    "def print_memory_usage(current_actions, episode):\n",
    "    \"\"\"\n",
    "    print GPU memory usage by CUDA\n",
    "    \"\"\"\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Reserved memory: {} GB\".format(round(r/1024**3,3)))\n",
    "    print(\"Allocated memory: {} GB\".format(round(a/1024**3,3)))\n",
    "    print(\"Amount free in reserved: {} GB\".format(round(f/1024**3,3)))\n",
    "\n",
    "def print_state(action, reward, q, loss):\n",
    "    print(\"action: \", action)\n",
    "    print(\"reward: \", reward)\n",
    "    print(\"estimated q: \", q)\n",
    "    print(\"loss: \", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model/agent init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(\"checkpoints\") \n",
    "now = dt.datetime.now().strftime('%B-%d-%Y')\n",
    "model_path = save_dir / f\"saved_agents/with_position/{now}/\"\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    os.mkdir(model_path / \"logs\")\n",
    "    f = open(model_path/ \"logs/log\", \"w\")\n",
    "    f.close()\n",
    "agent = Agent47(state_dim=3, action_dim=7, save_dir=save_dir, load=False, \n",
    "    load_file='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape torch.Size([16, 3, 2, 60])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-fbc6909d5985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"online\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"online\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-1fcf34ecca11>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, model)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'online'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "state = np.array(get_state(), dtype=\"float64\").T\n",
    "state = state.__array__() \n",
    "state = torch.tensor(state).cuda()\n",
    "state = state.unsqueeze(0)\n",
    "state = torch.movedim(state, 3, 1)\n",
    "state = torch.cat(agent.batch_size * [state])\n",
    "test_net = agent.net\n",
    "print(\"state shape\", state.shape)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "test_net.to(device)\n",
    "output = test_net(state, model=\"online\")\n",
    "summary(test_net, state, \"online\")\n",
    "print(test_net)\n",
    "print(\"output shape\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "logger = MetricLogger(model_path)\n",
    "actions = [partial(client.order, instrument_name=\"BTC-PERPETUAL\", side=\"long\", amount=2500, order_type=\"market\"), \n",
    "           partial(client.order, instrument_name=\"BTC-PERPETUAL\", side=\"short\", amount=2500, order_type=\"market\"),\n",
    "           partial(client.make_futures_order, side=\"long\", instrument=\"BTC-PERPETUAL\", amount=2500),\n",
    "           partial(client.make_futures_order, side=\"short\", instrument=\"BTC-PERPETUAL\", amount=2500),\n",
    "           partial(client.close_position, instrument=\"BTC-PERPETUAL\", order_type=\"limit\"),\n",
    "           client.cancel_all_orders,\n",
    "           client.do_nothing\n",
    "          ]\n",
    "\n",
    "episodes = 5\n",
    "action_num = 1000 #1000 actions in an episode\n",
    "equity = client.get_account_summary()[\"equity\"]\n",
    "\n",
    "for e in range(episodes): \n",
    "    #play the game:\n",
    "    step = 0\n",
    "    #get state from deribit\n",
    "    state = np.array(get_state(), dtype=\"float64\").T\n",
    "\n",
    "    while step < action_num:\n",
    "                \n",
    "        clear_output(wait=True)\n",
    "        print(\"Action {} in episode {}\".format(step + 1, e + 1))\n",
    "        print_memory_usage(step, e + 1) \n",
    "        \n",
    "        #agent runs on the state\n",
    "        action = agent.act(state) #action id\n",
    "        try:\n",
    "            print(\"Action index: {}\".format(action))\n",
    "            actions[action]()\n",
    "        except:\n",
    "            print(\"Empty or error, continuing until result\")\n",
    "            continue\n",
    "        try:\n",
    "            next_state = np.array(get_state(), dtype=\"float64\").T\n",
    "        except:\n",
    "            print(\"not enough order book values (probably). Continuing\")\n",
    "            continue\n",
    "            \n",
    "        reward = get_reward(equity) #reward calculated as total equity increase over the entire session\n",
    "        agent.cache(state, next_state, action, reward)\n",
    "        q, loss = agent.learn()\n",
    "        logger.log_step(reward, loss, q, action, step)\n",
    "        \n",
    "        state = next_state\n",
    "        print_state(action, reward, q, loss)\n",
    "        step += 1\n",
    "\n",
    "    logger.log_episode()\n",
    "    logger.record(episode=e, epsilon=agent.exploration_rate, step=agent.curr_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
