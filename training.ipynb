{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deribit market agent notebook v0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, time, datetime, os, copy\n",
    "import datetime as dt\n",
    "from api_client import Client\n",
    "from historical_data import get_historical_data\n",
    "from metric_logger import MetricLogger\n",
    "import nest_asyncio\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise trading client\n",
    "client = Client()\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エージェント neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectItem(nn.Module):\n",
    "    \"\"\"\n",
    "    for picking out an element from a tuple/list\n",
    "    at index item_index, for any layer which outputs such data\n",
    "    \"\"\"\n",
    "    def __init__(self, item_index):\n",
    "        super().__init__()\n",
    "        self._name = 'selectitem'\n",
    "        self.item_index = item_index\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, _ = inputs\n",
    "        return x\n",
    "\n",
    "\n",
    "class Agent47Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_dim, out_channels=100, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=100, out_channels=50, kernel_size=2, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(0,1),\n",
    "            nn.LSTM(input_size=3, num_layers=3, hidden_size=64),\n",
    "            SelectItem(1),\n",
    "            nn.Linear(64, output_dim) \n",
    "        )\n",
    "        \n",
    "        #frozen target Q\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "        \n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False   \n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, model):\n",
    "        inputs = inputs.float()\n",
    "        if model == 'online':\n",
    "            return self.online(inputs)\n",
    "        elif model == 'target':\n",
    "            return self.target(inputs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エージェント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent47:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = Agent47Net(self.state_dim, self.action_dim)\n",
    "        self.net = self.net.to(device)\n",
    "        \n",
    "        self.exploration_rate = 1\n",
    "        self.decay_rate = 0.99975\n",
    "        self.min_rate = 0.1\n",
    "        self.curr_step = 0\n",
    "        \n",
    "        self.save_every = 100\n",
    "        \n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 1\n",
    "        \n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        \n",
    "        self.learn_every = 1  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 100  # no. of experiences between Q_target & Q_online sync\n",
    "        \n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Return the action to do at given state\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_id = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = state.__array__() \n",
    "            state = torch.tensor(state).cuda()\n",
    "            state = state.unsqueeze(0)\n",
    "            action_vals = self.net(state, model=\"online\") #action scores from NN\n",
    "            action_id = torch.argmax(action_vals, axis=1).item() #value of action with highest score\n",
    "        \n",
    "        if self.exploration_rate >= self.min_rate:\n",
    "            self.exploration_rate -= 1-self.decay_rate #linear decay\n",
    "        \n",
    "        self.curr_step += 1\n",
    "        return action_id #action to take at this step\n",
    "    \n",
    "    def cache(self, state, next_state, action, reward):\n",
    "        \"\"\"\n",
    "        Store experience to memory\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "        \n",
    "        state = torch.tensor(state).cuda()\n",
    "        next_state = torch.tensor(next_state).cuda()\n",
    "        action = torch.tensor([action]).cuda()\n",
    "        reward = torch.tensor([reward]).cuda()\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward))\n",
    "    \n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve/\"remember\" experiences\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze()       \n",
    "    \n",
    "    def td_estimate(self, state, action):\n",
    "        \"\"\"\n",
    "        temporal difference Q estimate at current state\n",
    "        \"\"\"\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]\n",
    "        return current_Q\n",
    "\n",
    "    def td_target(self, reward, next_state):\n",
    "        \"\"\"\n",
    "        get td target\n",
    "        \"\"\"\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (self.gamma * next_Q).float())     \n",
    "    \n",
    "    def update_Q(self, td_estimate, td_target):\n",
    "        \"\"\"\n",
    "        update parameters\n",
    "        \"\"\"\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def sync_Q_target(self):\n",
    "        \"\"\"\n",
    "        sync Q target with online periodically instead of backpropagation\n",
    "        \"\"\"\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        save model in directory\n",
    "        \"\"\"\n",
    "        save_path = (\n",
    "            self.save_dir / f\"agent47_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"Agent47 saved to {save_path} at step {self.curr_step}\")\n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Learning steps\n",
    "        \"\"\"\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "            \n",
    "        if self.curr_step < 1 * self.batch_size:\n",
    "            return None, None\n",
    "        \n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state)\n",
    "\n",
    "        # Backpropagate loss through Q_online \n",
    "        loss = self.update_Q(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss) #(estimate of Q, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "NVIDIA GeForce RTX 3080\n",
      "Device total memory: 9.75 GB\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"Device total memory: {} GB\".format(round(t/1024**3,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(equity1):\n",
    "    \"\"\"\n",
    "    return reward value\n",
    "    \"\"\"\n",
    "    summary = client.get_account_summary()\n",
    "    equity2 = float(summary[\"equity\"])\n",
    "    return (equity2-equity1)\n",
    "    \n",
    "def get_state():\n",
    "    \"\"\"\n",
    "    get state at time of function call\n",
    "    currently only order book\n",
    "    \"\"\"\n",
    "    curr_time = round(time.time())\n",
    "    now = dt.datetime.now().strftime('%B %d, %Y %H:%M:%S')\n",
    "    tick_data = get_historical_data(t1=curr_time-120, t2=curr_time)\n",
    "    order_book = client.get_order_book(instrument='BTC-PERPETUAL', depth=50)\n",
    "    bids = order_book['bids']\n",
    "    asks = order_book['asks']\n",
    "    return [bids, asks]\n",
    "\n",
    "def print_memory_usage(current_actions, episode):\n",
    "    \"\"\"\n",
    "    print GPU memory usage by CUDA\n",
    "    \"\"\"\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    clear_output(wait=True)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Reserved memory: {} GB\".format(round(r/1024**3,3)))\n",
    "    print(\"Allocated memory: {} GB\".format(round(a/1024**3,3)))\n",
    "    print(\"Amount free in reserved: {} GB\".format(round(f/1024**3,3)))\n",
    "    print(\"Action {} in episode {}\".format(current_actions, episode))\n",
    "\n",
    "def print_state(action, reward, q, loss, state):\n",
    "    print(\"action: \", action)\n",
    "    print(\"reward: \", reward)\n",
    "    print(\"estimated q: \", q)\n",
    "    print(\"loss: \", loss)\n",
    "    print(\"state/orderbook: \", state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 27, 6]               --\n",
      "|    └─Conv2d: 2-1                       [-1, 100, 51, 3]          900\n",
      "|    └─ReLU: 2-2                         [-1, 100, 51, 3]          --\n",
      "|    └─Conv2d: 2-3                       [-1, 50, 27, 3]           20,050\n",
      "|    └─ReLU: 2-4                         [-1, 50, 27, 3]           --\n",
      "|    └─Flatten: 2-5                      [-1, 27, 3]               --\n",
      "|    └─LSTM: 2-6                         [-1, 27, 64]              84,224\n",
      "|    └─SelectItem: 2-7                   [-1, 27, 64]              --\n",
      "|    └─Linear: 2-8                       [-1, 27, 6]               390\n",
      "==========================================================================================\n",
      "Total params: 105,564\n",
      "Trainable params: 105,564\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 1.93\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.16\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 0.57\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 27, 6]               --\n",
       "|    └─Conv2d: 2-1                       [-1, 100, 51, 3]          900\n",
       "|    └─ReLU: 2-2                         [-1, 100, 51, 3]          --\n",
       "|    └─Conv2d: 2-3                       [-1, 50, 27, 3]           20,050\n",
       "|    └─ReLU: 2-4                         [-1, 50, 27, 3]           --\n",
       "|    └─Flatten: 2-5                      [-1, 27, 3]               --\n",
       "|    └─LSTM: 2-6                         [-1, 27, 64]              84,224\n",
       "|    └─SelectItem: 2-7                   [-1, 27, 64]              --\n",
       "|    └─Linear: 2-8                       [-1, 27, 6]               390\n",
       "==========================================================================================\n",
       "Total params: 105,564\n",
       "Trainable params: 105,564\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.93\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.16\n",
       "Params size (MB): 0.40\n",
       "Estimated Total Size (MB): 0.57\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "state = np.array(get_state(), dtype=\"float64\").T\n",
    "state = state.__array__() \n",
    "state = torch.tensor(state).cuda()\n",
    "state = state.unsqueeze(0)\n",
    "summary(Agent47Net(2,6), state, \"online\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3080\n",
      "Reserved memory: 0.43 GB\n",
      "Allocated memory: 0.047 GB\n",
      "Amount free in reserved: 0.383 GB\n",
      "Action 93 in episode 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-09c00714470b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#agent runs on the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#action id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-8deae0c6fd0b>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0maction_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"online\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#action scores from NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0maction_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#value of action with highest score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_rate\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "save_dir = Path(\"checkpoints\") \n",
    "agent = Agent47(state_dim=2, action_dim=6, save_dir=save_dir)\n",
    "logger = MetricLogger(save_dir)\n",
    "actions = [partial(client.order, instrument_name=\"BTC-PERPETUAL\", side=\"long\", amount=500, order_type=\"market\"), \n",
    "           partial(client.order, instrument_name=\"BTC-PERPETUAL\", side=\"short\", amount=500, order_type=\"market\"),\n",
    "           partial(client.make_futures_order, side=\"long\", instrument=\"BTC-PERPETUAL\", amount=500),\n",
    "           partial(client.make_futures_order, side=\"short\", instrument=\"BTC-PERPETUAL\", amount=500),\n",
    "           client.cancel_all_orders,\n",
    "           client.do_nothing\n",
    "          ]\n",
    "\n",
    "episodes = 5 \n",
    "action_num = 1000 #1000 actions in an episode\n",
    "\n",
    "for i in range(episodes): \n",
    "    #play the game:\n",
    "    current_actions = 0\n",
    "    \n",
    "    state = np.array(get_state(), dtype=\"float64\").T\n",
    "    #get state from deribit\n",
    "    equity = client.get_account_summary()[\"equity\"]\n",
    "    equity_list = [0]*200\n",
    "\n",
    "    while current_actions < action_num:\n",
    "        \n",
    "        #agent runs on the state\n",
    "        action = agent.act(state) #action id\n",
    "        try:\n",
    "            actions[action]()\n",
    "        except:\n",
    "            print(\"Empty or error, continuing until result\")\n",
    "            continue\n",
    "        try:\n",
    "            next_state = np.array(get_state(), dtype=\"float64\").T\n",
    "        except:\n",
    "            print(\"not enough order book values (probably). Continuing\")\n",
    "            continue\n",
    "            \n",
    "        reward = get_reward(equity) #reward calculated as total equity increase over episode\n",
    "        agent.cache(state, next_state, action, reward)\n",
    "        q, loss = agent.learn()\n",
    "        logger.log_step(reward, loss, q, action, current_actions)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        current_actions += 1\n",
    "        print_state(action, reward, q, loss, state)\n",
    "        print_memory_usage(current_actions, i + 1)\n",
    "\n",
    "    logger.log_episode()\n",
    "    logger.record(episode=e, epsilon=agent.exploration_rate, step=agent.curr_step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
